{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing, GridSearch and Training F-DenseNet-6 on Private Data without B/W-tests features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import normaltest, kstest, ttest_ind, ttest_rel, mannwhitneyu, wilcoxon, levene, bartlett\n",
    "from scipy.stats import chi2_contingency, fisher_exact, mode, pearsonr, f_oneway, kruskal, spearmanr\n",
    "\n",
    "from rfpimp import *\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "from seaborn import heatmap\n",
    "import random\n",
    "import statsmodels.distributions.empirical_distribution as edf\n",
    "from scipy.interpolate import interp1d\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, log_loss, recall_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, PowerTransformer\n",
    "\n",
    "from sklearn.utils import check_consistent_length, column_or_1d, assert_all_finite\n",
    "from sklearn.utils.extmath import stable_cumsum\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import os\n",
    "\n",
    "#from sklearn.externals import joblib\n",
    "import joblib\n",
    "\n",
    "from importlib import reload\n",
    "import Filter_and_Grid_Search\n",
    "Filter_and_Grid_Search = reload(Filter_and_Grid_Search)\n",
    "from Filter_and_Grid_Search import stratified_split\n",
    "from Filter_and_Grid_Search import attributes_list, attributes_list_new\n",
    "from Filter_and_Grid_Search import get_s_stat, get_PSI_stat, get_stats_by_month, get_stats, stable_unstable\n",
    "from Filter_and_Grid_Search import stable_unstable_by_month_divide, union_datas, individual_hists_all \n",
    "from Filter_and_Grid_Search import paired_time_hists_by_month, statistics_with_target\n",
    "from Filter_and_Grid_Search import receive_correlations, find_doubles_corr\n",
    "from Filter_and_Grid_Search import statistics_with_target, attributes_list, attributes_list_new, make_standard\n",
    "from Filter_and_Grid_Search import data_preprocessing_train, data_preprocessing_test\n",
    "from Filter_and_Grid_Search import receive_correlations, find_doubles_corr\n",
    "from Filter_and_Grid_Search import stratified_split, two_forests, turn_variables_with_values\n",
    "from Filter_and_Grid_Search import find_meta_params, calculate_vif#, find_meta_params_mem\n",
    "from Filter_and_Grid_Search import plot_meta_2d, data_preprocessing, find_ouliers_iqr\n",
    "from Filter_and_Grid_Search import train_model_receive_stats, simple_b_score_risk\n",
    "from Filter_and_Grid_Search import max_prof_corve, by_month_gini, check_attribute_list_cases\n",
    "\n",
    "from Filter_and_Grid_Search import to_zip, br_correction, br_stat\n",
    "\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import pathlib\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'D:/Share/safanasev/Python-notebook/AF_ML_v2_2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'BAD_FLAG'\n",
    "index_month = 'MONTH_YEAR'\n",
    "list_of_vars_for_strat = ['MONTH_YEAR']\n",
    "sort_by_var = 'APPPOSID'\n",
    "\n",
    "necessary_fields = [target, index_month, sort_by_var]\n",
    "\n",
    "COL_DEL = ['Unnamed: 0', 'PERIOD_7', 'LOSS_90P'] \n",
    "COL_DEL = [x.upper() for x in COL_DEL]\n",
    "COL_TRG = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружаем данные для 163 переменных\n",
    "\n",
    "train_for = pd.read_csv(PATH + 'train_163_prep.csv')\n",
    "valid_for = pd.read_csv(PATH + 'valid_163_prep.csv')\n",
    "test_for = pd.read_csv(PATH + 'test_163_prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_for[target]\n",
    "#y_test = valid_for[target]\n",
    "#y_val = test_for[target]\n",
    "\n",
    "y_test = test_for[target]\n",
    "y_val = valid_for[target]\n",
    "\n",
    "train_for.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "valid_for.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "test_for.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "col = train_for.columns.to_list()\n",
    "col.remove(target)\n",
    "\n",
    "X_1_2 = train_for[col]\n",
    "X_2_2 = valid_for[col]\n",
    "X_3_2 = test_for[col]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing B-tests and W-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_test = []\n",
    "w_test = []\n",
    "del_all = []\n",
    "\n",
    "for i in X_2_2.columns:\n",
    "    if i.find('W_TEST') >= 0:\n",
    "        w_test.append(i)\n",
    "        del_all.append(i)\n",
    "    if i.find('B_TEST') >= 0:\n",
    "        b_test.append(i)\n",
    "        del_all.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = train_for.columns.to_list()\n",
    "col.remove(target)\n",
    "col_2 = [i for i in col if i not in del_all]\n",
    "X_1_2 = train_for[col_2]\n",
    "X_2_2 = valid_for[col_2]\n",
    "X_3_2 = test_for[col_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1507599, 124), (1507599, 164), 1507599)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#проверка корректности\n",
    "X_1_2.shape, train_for.shape, len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#проверка корректности, должно быть везде True\n",
    "print(X_1_2.shape[0] == train_for.shape[0])\n",
    "print(len(y_train) == train_for.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#проверка корректности, должно быть везде True\n",
    "print(X_2_2.shape[0] == valid_for.shape[0])\n",
    "print(len(y_val) == valid_for.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = X_2_2.columns\n",
    "y_col = 'BAD_FLAG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'binary'\n",
    "missing_strings = 'MISSING'\n",
    "p_value = 0.05\n",
    "target_dict = {'good': 0, 'bad': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_2 = np_utils.to_categorical(y_val, 2) # преобразовываем в 2 класса "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((185400, 124), (185400, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2_2.shape, Y_test_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Activation, Dense, Flatten, Dropout)\n",
    "from tensorflow.keras.layers import (Conv1D, MaxPooling1D, AveragePooling1D)\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import ReLU, Concatenate, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 29\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "# for later versions:\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#если alpha не задано, то не делаем br_correction\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self,  dataset, x_col, y_col,  batch_size=256, num_classes=2, alpha = 0.1, random_state = 42, \n",
    "                 shuffle=True, class_w = None):\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "        self.indices = self.dataset.index.tolist()\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.alpha = alpha\n",
    "        self.random_state = random_state\n",
    "        self.class_w = class_w\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.indices) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.indices[k] for k in index]\n",
    "        \n",
    "        data_tmp = self.dataset.loc[batch]\n",
    "        data_tmp_b = data_tmp[data_tmp[self.y_col] == 1]\n",
    "        data_tmp_b_all = self.dataset[self.dataset[self.y_col] == 1]\n",
    "        X_tmp = data_tmp.head(0)\n",
    "        \n",
    "        \n",
    "        #print(data_tmp_b.shape[0], data_tmp.shape[0], data_tmp_b_all.shape[0])\n",
    "        if (self.alpha is None) and (self.class_w is None):\n",
    "            X = data_tmp[self.x_col] #.reshape(-1)\n",
    "            y = data_tmp[self.y_col] \n",
    "            \n",
    "        elif (self.alpha is None) and self.class_w > 0:\n",
    "            k = (self.class_w*self.batch_size)/(1-self.class_w)\n",
    "            k = k - data_tmp_b.shape[0]\n",
    "            if int(k) < 1:\n",
    "                k = k+1\n",
    "            ix2 = np.random.RandomState(self.random_state + int(k)).choice(data_tmp_b_all.shape[0], int(k))\n",
    "            X_tmp = data_tmp_b_all.iloc[ix2]\n",
    "            data_tmp = data_tmp.append(X_tmp, ignore_index=True)\n",
    "            X = data_tmp[self.x_col] #.reshape(-1)\n",
    "            y = data_tmp[self.y_col]\n",
    "            #print(data_tmp_b.shape[0], data_tmp.shape[0],  data_tmp_b.shape[0]/(data_tmp.shape[0] - X_tmp.shape[0]), np.mean(y)) \n",
    "            \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            if data_tmp_b.shape[0] > 0:\n",
    "                k=(self.alpha*self.batch_size/data_tmp_b.shape[0] -1 )/(1-self.alpha)\n",
    "                for i in range(0,int(k)):\n",
    "                    X_tmp = X_tmp.append(data_tmp_b, ignore_index=True)\n",
    "            else:\n",
    "                k = (self.alpha*self.batch_size)/(1-self.alpha)\n",
    "                ix2 = np.random.RandomState(self.random_state + int(k)).choice(data_tmp_b_all.shape[0], int(k))\n",
    "                X_tmp = data_tmp_b_all.iloc[ix2]\n",
    "                data_tmp_b = X_tmp\n",
    "\n",
    "    #         print('Добавим дробное число строк')    \n",
    "            k_fraction = k - int(k)\n",
    "            n_samples = int(round(data_tmp_b.shape[0]*k_fraction))\n",
    "    #         print(k_fraction, n_samples)\n",
    "\n",
    "            ix = np.random.RandomState(self.random_state).choice(data_tmp_b.shape[0], n_samples)\n",
    "            data_add_fraction = data_tmp_b.iloc[ix]\n",
    "            X_tmp.append(data_add_fraction, ignore_index=True)\n",
    "\n",
    "            data_tmp = data_tmp.append(X_tmp, ignore_index=True)\n",
    "\n",
    "\n",
    "            X = data_tmp[self.x_col] #.reshape(-1)\n",
    "            y = data_tmp[self.y_col]   \n",
    "        #print(sum(self.dataset.loc[batch][self.y_col])/len(index), np.mean(y))\n",
    "        \n",
    "        if self.num_classes > 1:\n",
    "            y = np_utils.to_categorical( y, self.num_classes)\n",
    "            \n",
    "        ####for tf 2.3.0, [np.array(X)], np.array(y)\n",
    "\n",
    "        #return [np.array(X).reshape(X.shape[0], X.shape[1], 1)], np.array(y)\n",
    "        return [np.array(X)], np.array(y)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.index = np.arange(len(self.indices))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "\n",
    "# объявляем класс метрик\n",
    "\n",
    "class E_time(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        tm = datetime.strftime(datetime.now(), \"%d.%m.%Y %H:%M:%S\")\n",
    "        print ('train_begin', '| time: ' , tm)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        tm = datetime.strftime(datetime.now(), \"%d.%m.%Y %H:%M:%S\")\n",
    "        print ('epoch_end', '| time: ' , tm)\n",
    "    \n",
    "        return\n",
    "\n",
    "_time = E_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, valid_data):\n",
    "        super(Metrics, self).__init__()\n",
    "        self.validation_data = valid_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_predict = self.model.predict(self.validation_data[0])[:, 1]\n",
    "        val_targ = self.validation_data[1]\n",
    "        \n",
    "        if len(val_targ.shape) == 2: #and val_targ.shape[1] != 1:\n",
    "            val_targ = val_targ[:,1]\n",
    "\n",
    "        _val_aps = metrics.average_precision_score(val_targ, val_predict)\n",
    "        #_val_recall = metrics.recall_score(val_targ, val_predict)\n",
    "        _val_a = metrics.roc_auc_score(val_targ, val_predict)\n",
    "\n",
    "        logs['val_aps'] = _val_aps\n",
    "        logs['val_a'] = _val_a\n",
    "        print(\" — val_aps:  %f — val_a: %f\" % (_val_aps, _val_a))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(min_pool, n_pool, padding_pool,  str_pool, y, str_min = 1):\n",
    "    #for 3D-tensor\n",
    "    if y.shape[1] < (min_pool):\n",
    "        \n",
    "        return MaxPooling1D(pool_size = n_pool, padding=padding_pool, strides=str_min)(y)\n",
    "    else:\n",
    "        return MaxPooling1D(pool_size = n_pool, padding=padding_pool, strides=str_pool)(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-DenseNet-6 (with poolings after each convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraud_dense_model_6_pool(l2_conv = None, reg = 1E-5 , reg_dense = 1E-5,\n",
    "                 _bias = True,  n_features = 163, n_pool = 2, n_kernel = 5, n_filters = 5, n_strides = 1,\n",
    "                 classes = 2, hidden = 64 , drop_out = 0.25, drop_out_conv = 0.001, drop_out_rate = 4 ,\n",
    "                 padding_pool = 'valid' ,\n",
    "                gl_pool_max = False):\n",
    "    \n",
    "    if reg == None:\n",
    "        l2_batch_gamma = None \n",
    "        l2_batch_betta = None\n",
    "    else:\n",
    "        l2_batch_gamma = l2(reg)\n",
    "        l2_batch_betta =l2(reg)\n",
    "    \n",
    "    if reg_dense == None:\n",
    "        kernel_regularizer = None\n",
    "    else:\n",
    "        kernel_regularizer = l2(reg)   \n",
    "    \n",
    "    \n",
    "    x = Input(shape=(  n_features, 1))\n",
    "    n = 0\n",
    "    y = Conv1D(filters=n_filters, kernel_size=n_kernel, strides=n_strides, padding='same', \n",
    "           use_bias=_bias, kernel_regularizer=l2_conv)(x) \n",
    "\n",
    "    y = BatchNormalization(gamma_regularizer=l2_batch_gamma ,beta_regularizer=l2_batch_betta)(y)\n",
    "    y = ReLU()(y)\n",
    "#y = Dropout(rate=drop_rate)(y)\n",
    "\n",
    "    y = MaxPooling1D(pool_size = n_pool, padding=padding_pool)(y) ##\n",
    "    shortcut1_2 = MaxPooling1D(pool_size = n_pool, padding=padding_pool)(y)## поправить\n",
    "\n",
    "\n",
    "    # второй conv-block\n",
    "    y = Conv1D(filters=n_filters*2, kernel_size=n_kernel, strides=n_strides, padding='same',\n",
    "              use_bias=_bias, kernel_regularizer=l2_conv)(y) \n",
    "    y = BatchNormalization(gamma_regularizer=l2_batch_gamma ,beta_regularizer=l2_batch_betta)(y)\n",
    "    y = ReLU()(y)\n",
    "\n",
    "    y = MaxPooling1D(pool_size = n_pool, padding=padding_pool)(y) ##\n",
    "\n",
    "\n",
    "    y= Concatenate(axis=-1)([shortcut1_2, y])\n",
    "    n = n+1\n",
    "\n",
    "    # третий conv-block\n",
    "    y = Conv1D(filters=n_filters*(3 + 1), kernel_size=n_kernel, strides=n_strides, padding='same',\n",
    "              use_bias=_bias, kernel_regularizer=l2_conv)(y) \n",
    "    y = BatchNormalization(gamma_regularizer=l2_batch_gamma ,beta_regularizer=l2_batch_betta)(y)\n",
    "    y = ReLU()(y)\n",
    "\n",
    "    y = MaxPooling1D(pool_size = n_pool, padding=padding_pool)(y) ##\n",
    "    n = n+1\n",
    "    \n",
    "    # четвертый conv-block\n",
    "    y = Conv1D(filters=n_filters*(4 + 1), kernel_size=n_kernel, strides=n_strides, padding='same',\n",
    "              use_bias=_bias, kernel_regularizer=l2_conv)(y) \n",
    "    y = BatchNormalization(gamma_regularizer=l2_batch_gamma ,beta_regularizer=l2_batch_betta)(y)\n",
    "    y = ReLU()(y)\n",
    "\n",
    "    y = MaxPooling1D(pool_size = n_pool, padding=padding_pool)(y) ##\n",
    "    shortcut4_6 = MaxPooling1D(pool_size = n_pool, padding=padding_pool)(y)##\n",
    "\n",
    "    # пятый conv-block\n",
    "    y = Conv1D(filters=n_filters*(5 + 1), kernel_size=n_kernel, strides=n_strides, padding='same',\n",
    "              use_bias=_bias, kernel_regularizer=l2_conv)(y) \n",
    "    y = BatchNormalization(gamma_regularizer=l2_batch_gamma ,beta_regularizer=l2_batch_betta)(y)\n",
    "    y = ReLU()(y)\n",
    "\n",
    "    y = MaxPooling1D(pool_size = n_pool, padding=padding_pool)(y) ##\n",
    "    y= Concatenate(axis=-1)([shortcut4_6,  y])\n",
    "\n",
    "    # шестой conv-block\n",
    "    y = Conv1D(filters=n_filters*(6 + 1), kernel_size=n_kernel, strides=n_strides, padding='same',\n",
    "              use_bias=_bias, kernel_regularizer=l2_conv)(y) \n",
    "    y = BatchNormalization(gamma_regularizer=l2_batch_gamma ,beta_regularizer=l2_batch_betta)(y)\n",
    "    y = ReLU()(y)\n",
    "    if gl_pool_max:\n",
    "        z = GlobalMaxPooling1D()(y)\n",
    "        \n",
    "    else:\n",
    "        z = GlobalAveragePooling1D()(y)\n",
    "    #z = Flatten()(y) #сглаживание, пример использования - https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras\n",
    "    z = Dense(hidden, activation='relu', kernel_regularizer = kernel_regularizer)(z)\n",
    "    z = Dropout(drop_out)(z)\n",
    "    z = Dense(hidden, activation='relu', kernel_regularizer = kernel_regularizer)(z)\n",
    "    z = Dropout(drop_out)(z)\n",
    "    predictions = Dense(classes, activation='softmax')(z)\n",
    "\n",
    "    #model = Sequential()\n",
    "    model_15 = Model(inputs=x, outputs=predictions)\n",
    "    \n",
    "    return model_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(min_pool, n_pool, padding_pool, str_pool, y ):\n",
    "    if y.shape[1] < (min_pool):\n",
    "        \n",
    "        return MaxPooling1D(pool_size = n_pool, padding=padding_pool, strides=1)(y)\n",
    "    else:\n",
    "        return MaxPooling1D(pool_size = n_pool, padding=padding_pool, strides=str_pool)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_p = [False] #0\n",
    "l2_batch = [ 0.0002] #1\n",
    "n_ker = [5] #2\n",
    "n_fil = [5] #3\n",
    "d_hidden = [60] #4\n",
    "drop_out = [0.25] #5\n",
    "reg_dense = [0.0002] #6\n",
    "min_pool = [20] #7\n",
    "# n_lay = [3, 6] #8  # это для CNN-сетки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = itertools.product(gl_p, l2_batch, n_ker, n_fil, d_hidden, drop_out, reg_dense, min_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_shape = X_2_2.shape[1]\n",
    "inp_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(train_for, x_col, y_col, batch_size=512, alpha = None, class_w = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-DenseNet-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 (185400, 124) (False, 0.0002, 5, 5, 60, 0.25, 0.0002, 20)\n",
      "WARNING:tensorflow:From <ipython-input-32-778e294b8fa3>:36: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "train_begin | time:  14.05.2021 16:49:41\n",
      "Epoch 1/150\n",
      "epoch_end | time:  14.05.2021 17:07:23\n",
      " — val_aps:  0.069135 — val_a: 0.926961\n",
      "2945/2945 - 1133s - loss: 0.0247 - accuracy: 0.9750 - auc: 0.9949 - precision: 0.9750 - recall: 0.9750 - val_loss: 0.0244 - val_accuracy: 0.9956 - val_auc: 0.9993 - val_precision: 0.9956 - val_recall: 0.9956\n",
      "Epoch 2/150\n",
      "epoch_end | time:  14.05.2021 17:25:24\n",
      " — val_aps:  0.068943 — val_a: 0.936289\n",
      "2945/2945 - 1062s - loss: 0.0115 - accuracy: 0.9892 - auc: 0.9978 - precision: 0.9892 - recall: 0.9892 - val_loss: 0.0261 - val_accuracy: 0.9943 - val_auc: 0.9989 - val_precision: 0.9943 - val_recall: 0.9943\n",
      "Epoch 3/150\n",
      "epoch_end | time:  14.05.2021 17:41:01\n",
      " — val_aps:  0.063613 — val_a: 0.921928\n",
      "2945/2945 - 948s - loss: 0.0100 - accuracy: 0.9907 - auc: 0.9983 - precision: 0.9907 - recall: 0.9907 - val_loss: 0.0189 - val_accuracy: 0.9965 - val_auc: 0.9990 - val_precision: 0.9965 - val_recall: 0.9965\n",
      "Epoch 4/150\n",
      "epoch_end | time:  14.05.2021 17:56:31\n",
      " — val_aps:  0.054589 — val_a: 0.932050\n",
      "2945/2945 - 944s - loss: 0.0092 - accuracy: 0.9914 - auc: 0.9985 - precision: 0.9914 - recall: 0.9914 - val_loss: 0.0186 - val_accuracy: 0.9965 - val_auc: 0.9992 - val_precision: 0.9965 - val_recall: 0.9965\n",
      "Epoch 5/150\n",
      "epoch_end | time:  14.05.2021 18:13:06\n",
      " — val_aps:  0.072466 — val_a: 0.933616\n",
      "2945/2945 - 993s - loss: 0.0087 - accuracy: 0.9918 - auc: 0.9987 - precision: 0.9918 - recall: 0.9918 - val_loss: 0.0367 - val_accuracy: 0.9901 - val_auc: 0.9982 - val_precision: 0.9901 - val_recall: 0.9901\n",
      "Epoch 6/150\n",
      "epoch_end | time:  14.05.2021 18:29:38\n",
      " — val_aps:  0.065621 — val_a: 0.938433\n",
      "2945/2945 - 992s - loss: 0.0083 - accuracy: 0.9925 - auc: 0.9989 - precision: 0.9925 - recall: 0.9925 - val_loss: 0.0296 - val_accuracy: 0.9943 - val_auc: 0.9992 - val_precision: 0.9943 - val_recall: 0.9943\n",
      "Epoch 7/150\n",
      "epoch_end | time:  14.05.2021 18:45:33\n",
      " — val_aps:  0.081768 — val_a: 0.933161\n",
      "2945/2945 - 957s - loss: 0.0079 - accuracy: 0.9929 - auc: 0.9990 - precision: 0.9929 - recall: 0.9929 - val_loss: 0.0226 - val_accuracy: 0.9945 - val_auc: 0.9992 - val_precision: 0.9945 - val_recall: 0.9945\n",
      "Epoch 8/150\n",
      "epoch_end | time:  14.05.2021 19:01:37\n",
      " — val_aps:  0.062548 — val_a: 0.920979\n",
      "2945/2945 - 930s - loss: 0.0078 - accuracy: 0.9931 - auc: 0.9990 - precision: 0.9931 - recall: 0.9931 - val_loss: 0.0469 - val_accuracy: 0.9882 - val_auc: 0.9978 - val_precision: 0.9882 - val_recall: 0.9882\n",
      "Epoch 9/150\n",
      "epoch_end | time:  14.05.2021 19:17:59\n",
      " — val_aps:  0.069050 — val_a: 0.929093\n",
      "2945/2945 - 1016s - loss: 0.0075 - accuracy: 0.9933 - auc: 0.9991 - precision: 0.9933 - recall: 0.9933 - val_loss: 0.0214 - val_accuracy: 0.9957 - val_auc: 0.9990 - val_precision: 0.9957 - val_recall: 0.9957\n",
      "Epoch 10/150\n",
      "epoch_end | time:  14.05.2021 19:33:26\n",
      " — val_aps:  0.064947 — val_a: 0.928461\n",
      "2945/2945 - 912s - loss: 0.0075 - accuracy: 0.9931 - auc: 0.9991 - precision: 0.9931 - recall: 0.9931 - val_loss: 0.0206 - val_accuracy: 0.9957 - val_auc: 0.9991 - val_precision: 0.9957 - val_recall: 0.9957\n",
      "Epoch 11/150\n",
      "epoch_end | time:  14.05.2021 19:48:51\n",
      " — val_aps:  0.059615 — val_a: 0.938430\n",
      "2945/2945 - 921s - loss: 0.0074 - accuracy: 0.9935 - auc: 0.9991 - precision: 0.9935 - recall: 0.9935 - val_loss: 0.0523 - val_accuracy: 0.9900 - val_auc: 0.9978 - val_precision: 0.9900 - val_recall: 0.9900\n",
      "Epoch 12/150\n",
      "epoch_end | time:  14.05.2021 20:04:31\n",
      " — val_aps:  0.047676 — val_a: 0.924982\n",
      "2945/2945 - 953s - loss: 0.0074 - accuracy: 0.9932 - auc: 0.9991 - precision: 0.9932 - recall: 0.9932 - val_loss: 0.0774 - val_accuracy: 0.9809 - val_auc: 0.9947 - val_precision: 0.9809 - val_recall: 0.9809\n",
      "Epoch 13/150\n",
      "epoch_end | time:  14.05.2021 20:20:29\n",
      " — val_aps:  0.066966 — val_a: 0.912625\n",
      "2945/2945 - 955s - loss: 0.0072 - accuracy: 0.9932 - auc: 0.9992 - precision: 0.9932 - recall: 0.9932 - val_loss: 0.0235 - val_accuracy: 0.9943 - val_auc: 0.9987 - val_precision: 0.9943 - val_recall: 0.9943\n",
      "Epoch 14/150\n",
      "epoch_end | time:  14.05.2021 20:35:37\n",
      " — val_aps:  0.068759 — val_a: 0.923382\n",
      "2945/2945 - 916s - loss: 0.0071 - accuracy: 0.9936 - auc: 0.9992 - precision: 0.9936 - recall: 0.9936 - val_loss: 0.0289 - val_accuracy: 0.9935 - val_auc: 0.9983 - val_precision: 0.9935 - val_recall: 0.9935\n",
      "Epoch 15/150\n",
      "epoch_end | time:  14.05.2021 20:51:46\n",
      " — val_aps:  0.019109 — val_a: 0.853772\n",
      "2945/2945 - 934s - loss: 0.0070 - accuracy: 0.9932 - auc: 0.9992 - precision: 0.9932 - recall: 0.9932 - val_loss: 0.5436 - val_accuracy: 0.8782 - val_auc: 0.9221 - val_precision: 0.8782 - val_recall: 0.8782\n",
      "Epoch 16/150\n",
      "epoch_end | time:  14.05.2021 21:07:41\n",
      " — val_aps:  0.072679 — val_a: 0.933229\n",
      "2945/2945 - 984s - loss: 0.0069 - accuracy: 0.9934 - auc: 0.9992 - precision: 0.9934 - recall: 0.9934 - val_loss: 0.0290 - val_accuracy: 0.9941 - val_auc: 0.9993 - val_precision: 0.9941 - val_recall: 0.9941\n",
      "Epoch 17/150\n",
      "epoch_end | time:  14.05.2021 21:23:05\n",
      " — val_aps:  0.058498 — val_a: 0.925979\n",
      "2945/2945 - 920s - loss: 0.0068 - accuracy: 0.9937 - auc: 0.9992 - precision: 0.9937 - recall: 0.9937 - val_loss: 0.0260 - val_accuracy: 0.9935 - val_auc: 0.9986 - val_precision: 0.9935 - val_recall: 0.9935\n",
      "Epoch 18/150\n",
      "epoch_end | time:  14.05.2021 21:38:01\n",
      " — val_aps:  0.063278 — val_a: 0.934245\n",
      "2945/2945 - 899s - loss: 0.0068 - accuracy: 0.9936 - auc: 0.9993 - precision: 0.9936 - recall: 0.9936 - val_loss: 0.0337 - val_accuracy: 0.9915 - val_auc: 0.9987 - val_precision: 0.9915 - val_recall: 0.9915\n",
      "Epoch 19/150\n",
      "epoch_end | time:  14.05.2021 21:52:57\n",
      " — val_aps:  0.067484 — val_a: 0.927712\n",
      "2945/2945 - 897s - loss: 0.0067 - accuracy: 0.9936 - auc: 0.9993 - precision: 0.9936 - recall: 0.9936 - val_loss: 0.0339 - val_accuracy: 0.9901 - val_auc: 0.9985 - val_precision: 0.9901 - val_recall: 0.9901\n",
      "Epoch 20/150\n",
      "epoch_end | time:  14.05.2021 22:08:31\n",
      " — val_aps:  0.062788 — val_a: 0.922134\n",
      "2945/2945 - 929s - loss: 0.0068 - accuracy: 0.9934 - auc: 0.9992 - precision: 0.9934 - recall: 0.9934 - val_loss: 0.0232 - val_accuracy: 0.9939 - val_auc: 0.9991 - val_precision: 0.9939 - val_recall: 0.9939\n",
      "Epoch 21/150\n",
      "epoch_end | time:  14.05.2021 22:24:01\n",
      " — val_aps:  0.061023 — val_a: 0.922950\n",
      "2945/2945 - 931s - loss: 0.0067 - accuracy: 0.9938 - auc: 0.9993 - precision: 0.9938 - recall: 0.9938 - val_loss: 0.0273 - val_accuracy: 0.9934 - val_auc: 0.9988 - val_precision: 0.9934 - val_recall: 0.9934\n",
      "Epoch 22/150\n",
      "epoch_end | time:  14.05.2021 22:40:18\n",
      " — val_aps:  0.056839 — val_a: 0.927695\n",
      "2945/2945 - 949s - loss: 0.0065 - accuracy: 0.9937 - auc: 0.9993 - precision: 0.9937 - recall: 0.9937 - val_loss: 0.0452 - val_accuracy: 0.9892 - val_auc: 0.9988 - val_precision: 0.9892 - val_recall: 0.9892\n",
      "Epoch 23/150\n",
      "epoch_end | time:  14.05.2021 22:55:53\n",
      " — val_aps:  0.061207 — val_a: 0.930233\n",
      "2945/2945 - 960s - loss: 0.0065 - accuracy: 0.9936 - auc: 0.9993 - precision: 0.9936 - recall: 0.9936 - val_loss: 0.0233 - val_accuracy: 0.9954 - val_auc: 0.9991 - val_precision: 0.9954 - val_recall: 0.9954\n",
      "Epoch 24/150\n",
      "epoch_end | time:  14.05.2021 23:12:03\n",
      " — val_aps:  0.058096 — val_a: 0.926092\n",
      "2945/2945 - 973s - loss: 0.0064 - accuracy: 0.9938 - auc: 0.9993 - precision: 0.9938 - recall: 0.9938 - val_loss: 0.0362 - val_accuracy: 0.9901 - val_auc: 0.9984 - val_precision: 0.9901 - val_recall: 0.9901\n",
      "==============================================\n",
      "model_fDenseNet6_PrivateData_wo_B_tests_PL_v2_0.0002_5_5_60_0.0002_200514_23 valid_for_train:  0.8521849693305652 0.0580957899963629 | test:  0.8626721246683413 0.05453666264190285\n",
      "==============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "VERBOSE = 2\n",
    "BATCH_SIZE = 512\n",
    "NB_EPOCH = 150\n",
    "class_weighting = {0: 0.1, 1: 1}\n",
    "\n",
    "result_all_8 = pd.DataFrame()\n",
    "result_all_8['name_model'] = None\n",
    "result_all_8['params'] = None\n",
    "result_all_8['val_GINI'] = None\n",
    "result_all_8['val_APS'] = None\n",
    "result_all_8['test_GINI'] = None\n",
    "result_all_8['test_APS'] = None\n",
    "j = 0\n",
    "\n",
    "for p in param:\n",
    "    print(inp_shape, X_2_2.shape, p)\n",
    "\n",
    "    \n",
    "    ###############################\n",
    "    #Вставить нужную функцию!!!!!!!!\n",
    "    model_grid = fraud_dense_model_6_pool(l2_conv = None, reg = p[1] , reg_dense = p[6],\n",
    "                                          n_features = inp_shape, n_pool = 2, n_kernel = p[2], n_filters = p[3],\n",
    "                                          n_strides = 1, classes = 2, hidden = p[4], drop_out = 0.25,\n",
    "                                          drop_out_conv = 0.001, drop_out_rate = 4, padding_pool = 'valid',\n",
    "                                          gl_pool_max = False)\n",
    "\n",
    "    model_grid.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "                       metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "    history_XX = model_grid.fit_generator(generator=training_generator,  # training_aug,\n",
    "                                          # Y_test_2 = np_utils.to_categorical( y_val, 2)\n",
    "                                          validation_data=(X_2_2, Y_test_2),\n",
    "                                          epochs=NB_EPOCH, verbose=VERBOSE, class_weight=class_weighting,\n",
    "                                          callbacks=[_time, EarlyStopping(monitor='val_loss', patience=20),\n",
    "                                                     Metrics(valid_data=(X_2_2, Y_test_2))])\n",
    "\n",
    "    res_model_ = pd.DataFrame(\n",
    "        history_XX.history, columns=history_XX.history.keys())\n",
    "    dd = str(200000 + datetime.now().month*100 +\n",
    "             datetime.now().day) + '_' + str(datetime.now().hour)\n",
    "    name_m = 'model_fDenseNet6_PrivateData_wo_B_tests_PL_v2_' + str(p[1]) + '_' + str(\n",
    "        p[2]) + '_' + str(p[3]) + '_' + str(p[4]) + '_' + str(p[6]) + '_' + str(dd)\n",
    "\n",
    "    model_grid.save(name_m + '.h5')\n",
    "    res_model_.to_csv(name_m + '.csv')\n",
    "\n",
    "    predict_class_val = model_grid.predict(X_2_2)\n",
    "    APS = metrics.average_precision_score(y_val, predict_class_val[:,1])\n",
    "    GINI = 2*(metrics.roc_auc_score(y_val , predict_class_val[:,1])) - 1\n",
    "\n",
    "    predict_class_test = model_grid.predict(X_3_2)\n",
    "    APS_t = metrics.average_precision_score(y_test, predict_class_test[:,1])\n",
    "    GINI_t = 2*(metrics.roc_auc_score(y_test, predict_class_test[:,1])) - 1\n",
    "\n",
    "    result_all_8.at[j, 'name_model'] = name_m\n",
    "    result_all_8.at[j, 'params'] = str(p)\n",
    "    result_all_8.at[j, 'val_GINI'] = GINI\n",
    "    result_all_8.at[j, 'val_APS'] = APS\n",
    "    result_all_8.at[j, 'test_GINI'] = GINI_t\n",
    "    result_all_8.at[j, 'test_APS'] = APS_t\n",
    "\n",
    "    result_all_8.to_csv('model_fDenseNet6_PrivateData_wo_B_tests_PL_all_v2.csv')\n",
    "    j += 1\n",
    "    print('==============================================')\n",
    "    print(name_m, 'valid_for_train: ', GINI, APS, '| test: ', GINI_t, APS_t)\n",
    "    print('==============================================')\n",
    "    print('')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
