{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation all Models on Open Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import normaltest, kstest, ttest_ind, ttest_rel, mannwhitneyu, wilcoxon, levene, bartlett\n",
    "from scipy.stats import chi2_contingency, fisher_exact, mode, pearsonr, f_oneway, kruskal, spearmanr\n",
    "\n",
    "from rfpimp import *\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "from seaborn import heatmap\n",
    "import random\n",
    "import statsmodels.distributions.empirical_distribution as edf\n",
    "from scipy.interpolate import interp1d\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, log_loss, recall_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, PowerTransformer\n",
    "\n",
    "from sklearn.utils import check_consistent_length, column_or_1d, assert_all_finite\n",
    "from sklearn.utils.extmath import stable_cumsum\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import os\n",
    "\n",
    "#from sklearn.externals import joblib\n",
    "import joblib\n",
    "\n",
    "from importlib import reload\n",
    "import Filter_and_Grid_Search\n",
    "Filter_and_Grid_Search = reload(Filter_and_Grid_Search)\n",
    "from Filter_and_Grid_Search import stratified_split\n",
    "from Filter_and_Grid_Search import attributes_list, attributes_list_new\n",
    "from Filter_and_Grid_Search import get_s_stat, get_PSI_stat, get_stats_by_month, get_stats, stable_unstable\n",
    "from Filter_and_Grid_Search import stable_unstable_by_month_divide, union_datas, individual_hists_all \n",
    "from Filter_and_Grid_Search import paired_time_hists_by_month, statistics_with_target\n",
    "from Filter_and_Grid_Search import receive_correlations, find_doubles_corr\n",
    "from Filter_and_Grid_Search import statistics_with_target, attributes_list, attributes_list_new, make_standard\n",
    "from Filter_and_Grid_Search import data_preprocessing_train, data_preprocessing_test\n",
    "from Filter_and_Grid_Search import receive_correlations, find_doubles_corr\n",
    "from Filter_and_Grid_Search import stratified_split, two_forests, turn_variables_with_values\n",
    "from Filter_and_Grid_Search import find_meta_params, calculate_vif#, find_meta_params_mem\n",
    "from Filter_and_Grid_Search import plot_meta_2d, data_preprocessing, find_ouliers_iqr\n",
    "from Filter_and_Grid_Search import train_model_receive_stats, simple_b_score_risk\n",
    "from Filter_and_Grid_Search import max_prof_corve, by_month_gini, check_attribute_list_cases\n",
    "\n",
    "from Filter_and_Grid_Search import to_zip, br_correction, br_stat\n",
    "\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import pathlib\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aps_ci_logit(y_true, y_scores, epsilon = 0.05):\n",
    "    aps = average_precision_score(y_true, y_scores)\n",
    "    n = len(y_true)\n",
    "    \n",
    "    nu = np.log(aps/(1-aps))\n",
    "    se = ((n*aps*(1-aps))**((-1)*1/2))\n",
    "    norm_se = norm.ppf(1-epsilon/2)*se\n",
    "    \n",
    "    exp_lower = np.exp(nu-norm_se)\n",
    "    exp_upper = np.exp(nu+norm_se)\n",
    "    \n",
    "    lower = exp_lower/(1+exp_lower)\n",
    "    upper = exp_upper/(1+exp_upper)\n",
    "    \n",
    "    return (lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import auc_delong_xu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auc_delong_xu import auc_ci_Delong\n",
    "\n",
    "#auc, auc_var, ci = auc_ci_Delong(y_true=y_true, y_scores=y_score)\n",
    "\n",
    "#print('ROC AUC: %s, Conf.' % auc) \n",
    "#print('Confidence Interval: %s (95%% confidence)' % str(ci))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path, time\n",
    "#print(\"last modified: %s\" % time.ctime(os.path.getmtime(file)))\n",
    "#print(\"created: %s\" % time.ctime(os.path.getctime(file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame(columns = ['full_name', 'name', 'type', 'ctime_d', 'ctime', 'time_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for f in listdir('C:/Users/safanasev/Desktop/Python-scripts/Python-notebook/depp'): \n",
    "\n",
    "#listdir('C:/Users/asmirnova5/Desktop/Python-scripts/Python-notebook/depp/model_step1'):\n",
    "\n",
    "\n",
    "    if len( f.split('.')) > 1:\n",
    "        tt =  f.split('.')[len( f.split('.')) - 1]\n",
    "        tt0 = f.split('.' + tt)[0]\n",
    "    else:\n",
    "        tt = f.split('.')[0]\n",
    "        tt0 = f\n",
    "    ct_n = os.path.getctime('C:/Users/safanasev/Desktop/Python-scripts/Python-notebook/depp' + '/' + f)\n",
    "    ct = datetime.strftime(datetime.fromtimestamp(ct_n), \"%d.%m.%Y %H:%M:%S\")\n",
    "    #k = [f, tt, ct_n, ct]\n",
    "    t_n = int(datetime.fromtimestamp(ct_n).year*10000 + datetime.fromtimestamp(ct_n).month*100 + datetime.fromtimestamp(ct_n).day)\n",
    "    dd = dd.append({'full_name': f, 'name': tt0, 'type':tt, 'ctime_d': ct_n, 'ctime': ct, 'time_num':t_n}, ignore_index=True)\n",
    "    #print(f, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d1 = dd[((dd['type'] == 'h5')) & (dd['time_num'] >= 20200908)]\n",
    "d1 = dd[((dd['type'] == 'h5'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d2 = dd[(dd['type'] == 'csv')  & (dd['time_num'] >= 20200908)][['full_name', 'name' ]]\n",
    "d2 = dd[(dd['type'] == 'csv')][['full_name', 'name' ]]\n",
    "d2.columns = ['csv_name', 'name' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_all = pd.merge(d1, d2, how='left', on='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = ['model_spider6_china_grid_0.0002_7_15_30_0.0002_200420_15',\n",
    "             'model_spider6_china_grid_0.0002_5_15_30_0.0002_200419_0',\n",
    "             'model_spider8_china_grid_15_5_15_60_0.0002_200429_3',\n",
    "             'model_spider8_china_grid_20_5_15_100_0.0002_200503_18',\n",
    "             'model_cnn_china_grid_False_5_15_60_3_200428_18',\n",
    "             'model_cnn_china_grid_False_5_15_60_6_200429_19',\n",
    "             'model_cnn_china_grid_False_5_5_100_6_200428_12',\n",
    "             'model_cnn_china_grid_False_5_10_30_8_200501_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = [\n",
    "    'model_spider6_china_grid_0.0002_7_15_30_0.0002_200420_15' ,\n",
    "'model_spider6_grid_wo_dropconv_0.25_0tTrue_0.0001_3_10_100' ,\n",
    "'model_spider6_grid_wo_dropconv_None_NonetTrue_0.0001_3_10_100', \n",
    "\n",
    "'model_spider8_china_grid_15_5_15_60_0.0002_200429_3' , \n",
    "'model_spider8_internal_0.25_0t15_5_15_60_0.0002_200610_10' , \n",
    "'model_spider8_internal_None_Nonet15_5_15_60_0.0002_200608_17'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_best = dd_all[dd_all['name'].isin(best_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_best.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_name</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>ctime_d</th>\n",
       "      <th>ctime</th>\n",
       "      <th>time_num</th>\n",
       "      <th>csv_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>model_spider6_china_grid_0.0002_7_15_30_0.0002...</td>\n",
       "      <td>model_spider6_china_grid_0.0002_7_15_30_0.0002...</td>\n",
       "      <td>h5</td>\n",
       "      <td>1.618921e+09</td>\n",
       "      <td>20.04.2021 15:16:25</td>\n",
       "      <td>20210420</td>\n",
       "      <td>model_spider6_china_grid_0.0002_7_15_30_0.0002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>model_spider6_grid_wo_dropconv_0.25_0tTrue_0.0...</td>\n",
       "      <td>model_spider6_grid_wo_dropconv_0.25_0tTrue_0.0...</td>\n",
       "      <td>h5</td>\n",
       "      <td>1.623089e+09</td>\n",
       "      <td>07.06.2021 21:06:47</td>\n",
       "      <td>20210607</td>\n",
       "      <td>model_spider6_grid_wo_dropconv_0.25_0tTrue_0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>model_spider6_grid_wo_dropconv_None_NonetTrue_...</td>\n",
       "      <td>model_spider6_grid_wo_dropconv_None_NonetTrue_...</td>\n",
       "      <td>h5</td>\n",
       "      <td>1.623068e+09</td>\n",
       "      <td>07.06.2021 15:18:32</td>\n",
       "      <td>20210607</td>\n",
       "      <td>model_spider6_grid_wo_dropconv_None_NonetTrue_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>model_spider8_china_grid_15_5_15_60_0.0002_200...</td>\n",
       "      <td>model_spider8_china_grid_15_5_15_60_0.0002_200...</td>\n",
       "      <td>h5</td>\n",
       "      <td>1.619658e+09</td>\n",
       "      <td>29.04.2021 03:52:07</td>\n",
       "      <td>20210429</td>\n",
       "      <td>model_spider8_china_grid_15_5_15_60_0.0002_200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>model_spider8_internal_0.25_0t15_5_15_60_0.000...</td>\n",
       "      <td>model_spider8_internal_0.25_0t15_5_15_60_0.000...</td>\n",
       "      <td>h5</td>\n",
       "      <td>1.623311e+09</td>\n",
       "      <td>10.06.2021 10:36:27</td>\n",
       "      <td>20210610</td>\n",
       "      <td>model_spider8_internal_0.25_0t15_5_15_60_0.000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>model_spider8_internal_None_Nonet15_5_15_60_0....</td>\n",
       "      <td>model_spider8_internal_None_Nonet15_5_15_60_0....</td>\n",
       "      <td>h5</td>\n",
       "      <td>1.623163e+09</td>\n",
       "      <td>08.06.2021 17:29:22</td>\n",
       "      <td>20210608</td>\n",
       "      <td>model_spider8_internal_None_Nonet15_5_15_60_0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             full_name  \\\n",
       "112  model_spider6_china_grid_0.0002_7_15_30_0.0002...   \n",
       "168  model_spider6_grid_wo_dropconv_0.25_0tTrue_0.0...   \n",
       "169  model_spider6_grid_wo_dropconv_None_NonetTrue_...   \n",
       "174  model_spider8_china_grid_15_5_15_60_0.0002_200...   \n",
       "185  model_spider8_internal_0.25_0t15_5_15_60_0.000...   \n",
       "202  model_spider8_internal_None_Nonet15_5_15_60_0....   \n",
       "\n",
       "                                                  name type       ctime_d  \\\n",
       "112  model_spider6_china_grid_0.0002_7_15_30_0.0002...   h5  1.618921e+09   \n",
       "168  model_spider6_grid_wo_dropconv_0.25_0tTrue_0.0...   h5  1.623089e+09   \n",
       "169  model_spider6_grid_wo_dropconv_None_NonetTrue_...   h5  1.623068e+09   \n",
       "174  model_spider8_china_grid_15_5_15_60_0.0002_200...   h5  1.619658e+09   \n",
       "185  model_spider8_internal_0.25_0t15_5_15_60_0.000...   h5  1.623311e+09   \n",
       "202  model_spider8_internal_None_Nonet15_5_15_60_0....   h5  1.623163e+09   \n",
       "\n",
       "                   ctime  time_num  \\\n",
       "112  20.04.2021 15:16:25  20210420   \n",
       "168  07.06.2021 21:06:47  20210607   \n",
       "169  07.06.2021 15:18:32  20210607   \n",
       "174  29.04.2021 03:52:07  20210429   \n",
       "185  10.06.2021 10:36:27  20210610   \n",
       "202  08.06.2021 17:29:22  20210608   \n",
       "\n",
       "                                              csv_name  \n",
       "112  model_spider6_china_grid_0.0002_7_15_30_0.0002...  \n",
       "168  model_spider6_grid_wo_dropconv_0.25_0tTrue_0.0...  \n",
       "169  model_spider6_grid_wo_dropconv_None_NonetTrue_...  \n",
       "174  model_spider8_china_grid_15_5_15_60_0.0002_200...  \n",
       "185  model_spider8_internal_0.25_0t15_5_15_60_0.000...  \n",
       "202  model_spider8_internal_None_Nonet15_5_15_60_0....  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame(columns = ['full_name', 'name', 'type', 'ctime_d', 'ctime', 'time_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for f in listdir('C:/Users/safanasev/Desktop/Python-scripts/Python-notebook/depp/best_dense/Open'): \n",
    "\n",
    "    if len( f.split('.')) > 1:\n",
    "        tt =  f.split('.')[len( f.split('.')) - 1]\n",
    "        tt0 = f.split('.' + tt)[0]\n",
    "    else:\n",
    "        tt = f.split('.')[0]\n",
    "        tt0 = f\n",
    "    ct_n = os.path.getctime('C:/Users/safanasev/Desktop/Python-scripts/Python-notebook/depp/best_dense/Open' + '/' + f)\n",
    "    ct = datetime.strftime(datetime.fromtimestamp(ct_n), \"%d.%m.%Y %H:%M:%S\")\n",
    "    #k = [f, tt, ct_n, ct]\n",
    "    t_n = int(datetime.fromtimestamp(ct_n).year*10000 + datetime.fromtimestamp(ct_n).month*100 + datetime.fromtimestamp(ct_n).day)\n",
    "    dd = dd.append({'full_name': f, 'name': tt0, 'type':tt, 'ctime_d': ct_n, 'ctime': ct, 'time_num':t_n}, ignore_index=True)\n",
    "    #print(f, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = dd[((dd['type'] == 'h5'))]\n",
    "d2 = dd[(dd['type'] == 'csv')][['full_name', 'name' ]]\n",
    "d2.columns = ['csv_name', 'name' ]\n",
    "dd_all = pd.merge(d1, d2, how='left', on='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_name</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>ctime_d</th>\n",
       "      <th>ctime</th>\n",
       "      <th>time_num</th>\n",
       "      <th>csv_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model_dense_OpenData_c3c3_k10_t0.5_w3.h5</td>\n",
       "      <td>model_dense_OpenData_c3c3_k10_t0.5_w3</td>\n",
       "      <td>h5</td>\n",
       "      <td>1.620732e+09</td>\n",
       "      <td>11.05.2021 14:14:51</td>\n",
       "      <td>20210511</td>\n",
       "      <td>model_dense_OpenData_c3c3_k10_t0.5_w3.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model_dense_OpenData_c4c4_k10_t0.5_w5.h5</td>\n",
       "      <td>model_dense_OpenData_c4c4_k10_t0.5_w5</td>\n",
       "      <td>h5</td>\n",
       "      <td>1.620732e+09</td>\n",
       "      <td>11.05.2021 14:14:51</td>\n",
       "      <td>20210511</td>\n",
       "      <td>model_dense_OpenData_c4c4_k10_t0.5_w5.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model_fDenseNet6_china_grid_0.0002_5_15_100_0....</td>\n",
       "      <td>model_fDenseNet6_china_grid_0.0002_5_15_100_0....</td>\n",
       "      <td>h5</td>\n",
       "      <td>1.620732e+09</td>\n",
       "      <td>11.05.2021 14:14:51</td>\n",
       "      <td>20210511</td>\n",
       "      <td>model_fDenseNet6_china_grid_0.0002_5_15_100_0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model_fDenseNet8_china_grid_0.0002_7_15_100_0....</td>\n",
       "      <td>model_fDenseNet8_china_grid_0.0002_7_15_100_0....</td>\n",
       "      <td>h5</td>\n",
       "      <td>1.620732e+09</td>\n",
       "      <td>11.05.2021 14:14:52</td>\n",
       "      <td>20210511</td>\n",
       "      <td>model_fDenseNet8_china_grid_0.0002_7_15_100_0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_name  \\\n",
       "0           model_dense_OpenData_c3c3_k10_t0.5_w3.h5   \n",
       "1           model_dense_OpenData_c4c4_k10_t0.5_w5.h5   \n",
       "2  model_fDenseNet6_china_grid_0.0002_5_15_100_0....   \n",
       "3  model_fDenseNet8_china_grid_0.0002_7_15_100_0....   \n",
       "\n",
       "                                                name type       ctime_d  \\\n",
       "0              model_dense_OpenData_c3c3_k10_t0.5_w3   h5  1.620732e+09   \n",
       "1              model_dense_OpenData_c4c4_k10_t0.5_w5   h5  1.620732e+09   \n",
       "2  model_fDenseNet6_china_grid_0.0002_5_15_100_0....   h5  1.620732e+09   \n",
       "3  model_fDenseNet8_china_grid_0.0002_7_15_100_0....   h5  1.620732e+09   \n",
       "\n",
       "                 ctime  time_num  \\\n",
       "0  11.05.2021 14:14:51  20210511   \n",
       "1  11.05.2021 14:14:51  20210511   \n",
       "2  11.05.2021 14:14:51  20210511   \n",
       "3  11.05.2021 14:14:52  20210511   \n",
       "\n",
       "                                            csv_name  \n",
       "0          model_dense_OpenData_c3c3_k10_t0.5_w3.csv  \n",
       "1          model_dense_OpenData_c4c4_k10_t0.5_w5.csv  \n",
       "2  model_fDenseNet6_china_grid_0.0002_5_15_100_0....  \n",
       "3  model_fDenseNet8_china_grid_0.0002_7_15_100_0....  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_best = pd.concat([dd_all, dd_best], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 7)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_best.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'LABEL'\n",
    "index_month = 'MONTH'\n",
    "list_of_vars_for_strat = ['MONTH']\n",
    "sort_by_var = 'ID'\n",
    "\n",
    "necessary_fields = [target, index_month, sort_by_var]\n",
    "\n",
    "COL_DEL = ['Unnamed: 0'] \n",
    "COL_DEL = [x.upper() for x in COL_DEL]\n",
    "COL_TRG = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'D:/Share/safanasev/Python-notebook/AF_ML_chinese/'\n",
    "train_for = pd.read_csv(PATH + 'china_train_128_prep.csv')\n",
    "valid_for = pd.read_csv(PATH + 'china_val_128_prep.csv')\n",
    "test_for = pd.read_csv(PATH + 'china_test_128_prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_for[target]\n",
    "y_val = valid_for[target]\n",
    "y_test = test_for[target]\n",
    "\n",
    "train_for.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "valid_for.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "test_for.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "col = train_for.columns.to_list()\n",
    "col.remove(target)\n",
    "\n",
    "\n",
    "X_1_2 = train_for[col]\n",
    "X_2_2 = valid_for[col]\n",
    "X_3_2 = test_for[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((792004, 128), (99001, 128), (99001, 128))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1_2.shape, X_2_2.shape, X_3_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_best['Val_APS'] = None\n",
    "dd_best['Val_APS_conf_l'] = None\n",
    "dd_best['Val_APS_conf_r'] = None\n",
    "dd_best['Val_GINI'] = None\n",
    "dd_best['Val_AUC'] = None\n",
    "dd_best['Val_AUC_conf_l'] = None\n",
    "dd_best['Val_AUC_conf_r'] = None\n",
    "\n",
    "dd_best['TEST_APS'] = None\n",
    "dd_best['TEST_APS_conf_l'] = None\n",
    "dd_best['TEST_APS_conf_r'] = None\n",
    "dd_best['TEST_GINI'] = None\n",
    "dd_best['TEST_AUC'] = None\n",
    "dd_best['TEST_AUC_conf_l'] = None\n",
    "dd_best['TEST_AUC_conf_r'] = None\n",
    "\n",
    "dd_best['Count_epoch'] = None\n",
    "dd_best['Count_feature'] = None\n",
    "dd_best['Min_loss'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_best['LR_schedule'] = None\n",
    "\n",
    "dd_best['saved_money_test'] = None\n",
    "dd_best['saved_money_val'] = None\n",
    "dd_best['saved_money_oot'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_best['Val_APS_conf_int'] = None\n",
    "dd_best['TEST_APS_conf_int'] = None\n",
    "dd_best['OOT2013_APS_conf_int'] = None\n",
    "\n",
    "dd_best['TEST_AUC_midle'] = None\n",
    "dd_best['TEST_AUC_conf_int'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Conv1D, Concatenate, AveragePooling1D\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "blocks/one_d.py\n",
    "Author: Ankit Gupta\n",
    "\n",
    "Implementations of various DenseNet blocks for 1D sequences\n",
    "\n",
    "This module contains helper functions that define the various subcomponents of a DenseNet. This includes dense blocks and transition blocks.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#from tf.keras.layers import BatchNormalization, Activation, Conv1D, Concatenate, AveragePooling1D\n",
    "\n",
    "\n",
    "def H_l(k, bottleneck_size, kernel_width):\n",
    "    \"\"\" \n",
    "    A single convolutional \"layer\" as defined by Huang et al. Defined as H_l in the original paper\n",
    "    \n",
    "    :param k: int representing the \"growth rate\" of the DenseNet\n",
    "    :param bottleneck_size: int representing the size of the bottleneck, as a multiple of k. Set to 0 for no bottleneck.\n",
    "    :param kernel_width: int representing the width of the main convolutional kernel\n",
    "    :return a function wrapping the keras layers for H_l\n",
    "    \"\"\"\n",
    "\n",
    "    use_bottleneck = bottleneck_size > 0\n",
    "    num_bottleneck_output_filters = k * bottleneck_size\n",
    "\n",
    "    def f(x):\n",
    "        if use_bottleneck:\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation(\"relu\")(x)\n",
    "            x = Conv1D(\n",
    "                num_bottleneck_output_filters,\n",
    "                1,\n",
    "                strides=1,\n",
    "                padding=\"same\",\n",
    "                dilation_rate=1)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Conv1D(\n",
    "            k,\n",
    "            kernel_width,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            dilation_rate=1)(x)\n",
    "        return x\n",
    "    return f\n",
    "\n",
    "\n",
    "def dense_block(k, num_layers, kernel_width, bottleneck_size):\n",
    "    \"\"\"\n",
    "    A single dense block of the DenseNet\n",
    "    \n",
    "    :param k: int representing the \"growth rate\" of the DenseNet\n",
    "    :param num_layers: int represending the number of layers in the block\n",
    "    :param kernel_width: int representing the width of the main convolutional kernel\n",
    "    :param bottleneck_size: int representing the size of the bottleneck, as a multiple of k. Set to 0 for no bottleneck.\n",
    "    :return a function wrapping the entire dense block\n",
    "    \"\"\"\n",
    "    def f(x):\n",
    "        layers_to_concat = [x]\n",
    "        for _ in range(num_layers):\n",
    "            x = H_l(k, bottleneck_size, kernel_width)(x)\n",
    "            layers_to_concat.append(x)\n",
    "            x = Concatenate(axis=-1)(layers_to_concat)\n",
    "        return x\n",
    "    return f\n",
    "\n",
    "\n",
    "def transition_block(pool_size=2, stride=2, theta=0.5):\n",
    "    \"\"\"\n",
    "    A single transition block of the DenseNet\n",
    "    \n",
    "    :param pool_size: int represending the width of the average pool\n",
    "    :param stride: int represending the stride of the average pool\n",
    "    :param theta: int representing the amount of compression in the 1x1 convolution. Set to 1 for no compression.\n",
    "    :return a function wrapping the entire transition block\n",
    "    \"\"\"    \n",
    "    assert theta > 0 and theta <= 1\n",
    "\n",
    "    def f(x):\n",
    "        num_transition_output_filters = int(int(x.shape[2]) * float(theta))\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Conv1D(\n",
    "            num_transition_output_filters,\n",
    "            1,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            dilation_rate=1)(x)\n",
    "        x = AveragePooling1D(\n",
    "            pool_size=pool_size,\n",
    "            strides=stride,\n",
    "            padding=\"same\")(x)\n",
    "        return x\n",
    "    return f\n",
    "\n",
    "\n",
    "def DenseNet_1(\n",
    "        k,\n",
    "        block_sizes,\n",
    "        conv_kernel_width,\n",
    "        bottleneck_size,\n",
    "        transition_pool_size,\n",
    "        transition_pool_stride,\n",
    "        theta,\n",
    "        initial_conv_width,\n",
    "        initial_stride,\n",
    "        initial_filters,\n",
    "        initial_pool_width,\n",
    "        initial_pool_stride,\n",
    "        use_global_pooling):\n",
    "    def f(x):\n",
    "        x = Conv1D(\n",
    "            initial_filters,\n",
    "            initial_conv_width,\n",
    "            strides=initial_stride,\n",
    "            padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = MaxPooling1D(\n",
    "            pool_size=initial_pool_width,\n",
    "            strides=initial_pool_stride,\n",
    "            padding=\"same\")(x)\n",
    "\n",
    "        # Add all but the last dense block\n",
    "        for block_size in block_sizes[:-1]:\n",
    "            x = dense_block(\n",
    "                k,\n",
    "                block_size,\n",
    "                conv_kernel_width,\n",
    "                bottleneck_size)(x)\n",
    "            x = transition_block(\n",
    "                pool_size=transition_pool_size,\n",
    "                stride=transition_pool_stride,\n",
    "                theta=theta)(x)\n",
    "\n",
    "        # Add the last dense block\n",
    "        final_block_size = block_sizes[-1]\n",
    "        x = dense_block(\n",
    "            k,\n",
    "            final_block_size,\n",
    "            conv_kernel_width,\n",
    "            bottleneck_size)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        if use_global_pooling:\n",
    "            x = GlobalAveragePooling1D()(x)\n",
    "        return x\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "class DenseNetCustom(tensorflow.keras.models.Model):\n",
    "    \"\"\"  \n",
    "    Create a Keras Model Object that is an implementation of DenseNet with a custom number of parameters. The number of layers \n",
    "    per dense block can be specified by block_sizes.\n",
    "    :param input_shape: The shape of the inputs without the batch dimension. This should be a valid 1D sequence, \n",
    "    such as (244, 25). \n",
    "    :param num_outputs: the number of classes to predict\n",
    "    :param k: The \"growth rate\" of the DenseNet model\n",
    "    :param block_sizes: A list of ints with the number of layers in each block. Example: [5, 10, 25, 17].\n",
    "    :param conv_kernel_width: The kernel width of each convolution in the dense blocks.\n",
    "    :param bottleneck_size: The size of the bottleneck, as a multiple of k. Set to 0 for no bottleneck.\n",
    "    :param transition_pool_size: pool_size in the transition layer\n",
    "    :param transition_pool_stride: pooling stride in the transition layer\n",
    "    :param theta: Amount of compression in the transition layer. Set to 1 for no compression.\n",
    "    :param initial_conv_width: Kernel width for the one convolution before the dense blocks\n",
    "    :param initial_stride: Stride for the one convolution before the dense blocks\n",
    "    :param initial_filters: Number of filters for the one convolution before the dense blocks\n",
    "    :param initial_pool_width: pool_size for the one pooling before the dense blocks\n",
    "    :param initial_pool_stride: stride for the one pooling before the dense blocks \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_shape,\n",
    "            num_outputs=1000,\n",
    "            k=32,\n",
    "            block_sizes=None,\n",
    "            conv_kernel_width=3,\n",
    "            bottleneck_size=4,\n",
    "            transition_pool_size=2,\n",
    "            transition_pool_stride=2,\n",
    "            theta=0.5,\n",
    "            initial_conv_width=7,\n",
    "            initial_stride=2,\n",
    "            initial_filters=64,\n",
    "            initial_pool_width=3,\n",
    "            initial_pool_stride=2):\n",
    "        if not block_sizes:\n",
    "            raise ValueError(\"block_sizes must be specified\")\n",
    "        model_input = Input(shape=input_shape)\n",
    "        output = DenseNet_1(\n",
    "            k,\n",
    "            block_sizes,\n",
    "            conv_kernel_width,\n",
    "            bottleneck_size,\n",
    "            transition_pool_size,\n",
    "            transition_pool_stride,\n",
    "            theta,\n",
    "            initial_conv_width,\n",
    "            initial_stride,\n",
    "            initial_filters,\n",
    "            initial_pool_width,\n",
    "            initial_pool_stride,\n",
    "            use_global_pooling=True)(model_input)\n",
    "        output = Dense(num_outputs, activation=\"softmax\")(output)\n",
    "        super(DenseNetCustom, self).__init__(model_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_all = dd_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 33)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 model_spider6_china_grid_0.0002_7_15_30_0.0002_200420_15.h5 begin | time:  11.06.2021 11:54:28\n",
      "168 model_spider6_grid_wo_dropconv_0.25_0tTrue_0.0001_3_10_100.h5 begin | time:  11.06.2021 11:55:25\n",
      "169 model_spider6_grid_wo_dropconv_None_NonetTrue_0.0001_3_10_100.h5 begin | time:  11.06.2021 11:55:47\n",
      "174 model_spider8_china_grid_15_5_15_60_0.0002_200429_3.h5 begin | time:  11.06.2021 11:56:11\n",
      "185 model_spider8_internal_0.25_0t15_5_15_60_0.0002_200610_10.h5 begin | time:  11.06.2021 11:57:37\n",
      "202 model_spider8_internal_None_Nonet15_5_15_60_0.0002_200608_17.h5 begin | time:  11.06.2021 11:59:06\n"
     ]
    }
   ],
   "source": [
    "#бизнес-метрика: на каждом периоде отбираем ТОП-ТТ и считаем по ним убыток\n",
    "#по данным из AF, в неделю проверяется 40 ТТ\n",
    "#поэтому для oot за 2013г берется 40, для test/ val берется 5, т.к. test/ val - это пот 10% от выборки\n",
    "#scores_optuna['saved_money_test_multiply'] = [saved_money_test_only_1*5] - 5 - это коэффициент, \n",
    "#который приводит результат к полным данным, для test/ val нужно брать 10\n",
    "\n",
    "\n",
    "for j in dd_all.index:\n",
    "    \n",
    "    \n",
    "    mod = dd_all['full_name'][j]\n",
    "    tm = datetime.strftime(datetime.now(), \"%d.%m.%Y %H:%M:%S\")\n",
    "    print (j, mod, 'begin', '| time: ' , tm)\n",
    "    \n",
    "    if mod.find('_dense_') >= 0:\n",
    "        model_X = tf.keras.models.load_model(('C:/Users/safanasev/Desktop/Python-scripts/Python-notebook/depp/' +mod),\n",
    "                                             custom_objects={'DenseNetCustom':DenseNetCustom})\n",
    "    else:\n",
    "        model_X = load_model('C:/Users/safanasev/Desktop/Python-scripts/Python-notebook/depp/' +mod)\n",
    "    \n",
    "    if str(dd_all[dd_all['full_name'] == mod]['csv_name'].to_list()[0]) != 'nan':\n",
    "        history_XX = pd.read_csv('C:/Users/safanasev/Desktop/Python-scripts/Python-notebook/depp/' + \n",
    "                                 dd_all[dd_all['full_name'] == mod]['csv_name'].to_list()[0])\n",
    "    \n",
    "    test = pd.DataFrame.from_dict(model_X.get_config()['layers'])\n",
    "    \n",
    "    \n",
    "    #\"\"\"\n",
    "    if test[test['class_name'] =='InputLayer']['config'][0]['batch_input_shape'][1] == 128:\n",
    "        \n",
    "        X_2 = X_2_2.copy(deep=True)   \n",
    "        X_2['predict_class'] = model_X.predict(X_2_2)[:,1]\n",
    "\n",
    "        dd_all.at[j, 'Val_APS'] = metrics.average_precision_score(y_val, X_2['predict_class'])\n",
    "        dd_all.at[j, 'Val_GINI']  = 2*metrics.roc_auc_score(y_val, X_2['predict_class']) - 1\n",
    "        \n",
    "        \n",
    "        #####\n",
    "        ci_a = aps_ci_logit(y_val, X_2['predict_class'])\n",
    "        dd_all.at[j,'Val_APS_conf_l'] = ci_a[0]\n",
    "        dd_all.at[j,'Val_APS_conf_r'] = ci_a[1]\n",
    "        dd_all.at[j,'Val_APS_conf_int'] = (ci_a[1] - ci_a[0])/2\n",
    "        auc, auc_var, ci = auc_ci_Delong(y_true=y_val, y_scores=X_2['predict_class'])\n",
    "        dd_all.at[j,'Val_AUC'] = auc\n",
    "        dd_all.at[j,'Val_AUC_conf_l'] = ci[0]\n",
    "        dd_all.at[j,'Val_AUC_conf_r'] = ci[1]\n",
    "        dd_all.at[j,'Val_AUC_conf_int'] = (ci[1] - ci[0])/2\n",
    "        del X_2\n",
    "\n",
    "        ####### test\n",
    "        \n",
    "        X_3 = X_3_2.copy(deep=True)   \n",
    "        X_3['predict_class'] = model_X.predict(X_3_2)[:,1]\n",
    "        \n",
    "        dd_all.at[j, 'TEST_APS'] = metrics.average_precision_score(y_test, X_3['predict_class'])\n",
    "        dd_all.at[j, 'TEST_GINI']  = 2*metrics.roc_auc_score(y_test, X_3['predict_class']) - 1\n",
    "        \n",
    "        ci_a = aps_ci_logit(y_test, X_3['predict_class'])\n",
    "        dd_all.at[j,'TEST_APS_conf_l'] = ci_a[0]\n",
    "        dd_all.at[j,'TEST_APS_conf_r'] = ci_a[1]\n",
    "        \n",
    "        dd_all.at[j,'TEST_APS_conf_int'] = (ci_a[1] - ci_a[0])/2\n",
    "        auc, auc_var, ci = auc_ci_Delong(y_true=y_test, y_scores=X_3['predict_class'])\n",
    "        dd_all.at[j,'TEST_AUC'] = auc\n",
    "        dd_all.at[j,'TEST_AUC_conf_l'] = ci[0]\n",
    "        dd_all.at[j,'TEST_AUC_conf_r'] = ci[1]\n",
    "        dd_all.at[j,'TEST_AUC_conf_int'] = (ci[1] - ci[0])/2\n",
    "        del X_3\n",
    "        \n",
    "        \n",
    "\n",
    "    ####\n",
    "    dd_all.at[j, 'Count_epoch'] = history_XX.shape[0]\n",
    "    dd_all.at[j, 'Min_loss'] = min(history_XX['val_loss'])\n",
    "    dd_all.at[j, 'Count_feature'] = test[test['class_name'] =='InputLayer']['config'][0]['batch_input_shape'][1]\n",
    "    \n",
    "    if 'lr' in history_XX.columns:\n",
    "        dd_all.at[j, 'LR_schedule'] = len(np.unique(history_XX['lr']))\n",
    "        \n",
    "    dd_all.to_excel('BEST_china_conf_int_20210611.xlsx')\n",
    "                                                                        \n",
    "    #tm = datetime.strftime(datetime.now(), \"%d.%m.%Y %H:%M:%S\")\n",
    "    #print(j, mod, 'end', '| time: ' , tm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
